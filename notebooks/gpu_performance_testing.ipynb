{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9fdd5e",
   "metadata": {},
   "source": [
    "# GPU Performance Testing and Optimization\n",
    "\n",
    "This notebook provides tools for testing GPU performance and optimizing workloads in the research environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4786fad",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Check\n",
    "\n",
    "First, let's check if a GPU is available and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import custom utilities\n",
    "sys.path.append('..')\n",
    "from utils.gpu_utils import gpu_manager\n",
    "\n",
    "# Check GPU availability\n",
    "gpu_available = gpu_manager.check_gpu_availability()\n",
    "print(f\"GPU available: {gpu_available}\")\n",
    "\n",
    "# Get GPU information if available\n",
    "if gpu_available:\n",
    "    gpu_info = gpu_manager.get_gpu_info()\n",
    "    \n",
    "    print(f\"GPU Count: {len(gpu_info)}\")\n",
    "    for idx, gpu in enumerate(gpu_info):\n",
    "        print(f\"GPU {idx}: {gpu.get('name', 'Unknown')}\")\n",
    "        print(f\"  Memory: {gpu.get('memory_total_mb', 0)} MB\")\n",
    "else:\n",
    "    print(\"No GPU detected. Some tests will be skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4179a78",
   "metadata": {},
   "source": [
    "## 2. PyTorch GPU Performance Testing\n",
    "\n",
    "Let's test PyTorch matrix operations on GPU vs CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c272b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not installed. Skipping PyTorch tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da5ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pytorch_performance(sizes=[1000, 2000, 4000, 8000]):\n",
    "    if not TORCH_AVAILABLE or not torch.cuda.is_available():\n",
    "        print(\"PyTorch or CUDA not available. Skipping test.\")\n",
    "        return\n",
    "    \n",
    "    # Results storage\n",
    "    results = {\n",
    "        'sizes': sizes,\n",
    "        'cpu_times': [],\n",
    "        'gpu_times': [],\n",
    "        'speedups': []\n",
    "    }\n",
    "    \n",
    "    for size in sizes:\n",
    "        print(f\"Testing matrix multiplication with size {size}x{size}...\")\n",
    "        \n",
    "        # Create random matrices\n",
    "        a_cpu = torch.rand(size, size)\n",
    "        b_cpu = torch.rand(size, size)\n",
    "        \n",
    "        # CPU test\n",
    "        start = time.time()\n",
    "        c_cpu = torch.matmul(a_cpu, b_cpu)\n",
    "        cpu_time = time.time() - start\n",
    "        results['cpu_times'].append(cpu_time)\n",
    "        print(f\"  CPU time: {cpu_time:.4f} seconds\")\n",
    "        \n",
    "        # GPU test\n",
    "        a_gpu = a_cpu.cuda()\n",
    "        b_gpu = b_cpu.cuda()\n",
    "        \n",
    "        # Warm-up run\n",
    "        c_gpu = torch.matmul(a_gpu, b_gpu)\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Timed run\n",
    "        start = time.time()\n",
    "        c_gpu = torch.matmul(a_gpu, b_gpu)\n",
    "        torch.cuda.synchronize()\n",
    "        gpu_time = time.time() - start\n",
    "        results['gpu_times'].append(gpu_time)\n",
    "        print(f\"  GPU time: {gpu_time:.4f} seconds\")\n",
    "        \n",
    "        # Calculate speedup\n",
    "        speedup = cpu_time / gpu_time if gpu_time > 0 else float('inf')\n",
    "        results['speedups'].append(speedup)\n",
    "        print(f\"  Speedup: {speedup:.2f}x\")\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del a_gpu, b_gpu, c_gpu\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(sizes, results['cpu_times'], 'b-o', label='CPU')\n",
    "    plt.plot(sizes, results['gpu_times'], 'r-o', label='GPU')\n",
    "    plt.xlabel('Matrix Size')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('Matrix Multiplication Performance')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(sizes, results['speedups'], 'g-o')\n",
    "    plt.xlabel('Matrix Size')\n",
    "    plt.ylabel('Speedup (CPU time / GPU time)')\n",
    "    plt.title('GPU Speedup Factor')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the test with smaller matrices first\n",
    "# Uncomment to run the test\n",
    "# results = test_pytorch_performance(sizes=[1000, 2000, 4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3489775",
   "metadata": {},
   "source": [
    "## 3. TensorFlow GPU Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f857eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    TF_AVAILABLE = True\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"TensorFlow not installed. Skipping TensorFlow tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d14de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tensorflow_performance(sizes=[1000, 2000, 4000]):\n",
    "    if not TF_AVAILABLE:\n",
    "        print(\"TensorFlow not available. Skipping test.\")\n",
    "        return\n",
    "    \n",
    "    # Check for GPU availability in TensorFlow\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if not gpus:\n",
    "        print(\"No GPU available for TensorFlow. Skipping test.\")\n",
    "        return\n",
    "    \n",
    "    # Results storage\n",
    "    results = {\n",
    "        'sizes': sizes,\n",
    "        'cpu_times': [],\n",
    "        'gpu_times': [],\n",
    "        'speedups': []\n",
    "    }\n",
    "    \n",
    "    # Set memory growth to avoid OOM errors\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for size in sizes:\n",
    "        print(f\"Testing TensorFlow matrix multiplication with size {size}x{size}...\")\n",
    "        \n",
    "        # CPU test\n",
    "        with tf.device('/cpu:0'):\n",
    "            a_cpu = tf.random.normal([size, size])\n",
    "            b_cpu = tf.random.normal([size, size])\n",
    "            \n",
    "            # Warm-up run\n",
    "            c_cpu = tf.matmul(a_cpu, b_cpu)\n",
    "            \n",
    "            # Timed run\n",
    "            start = time.time()\n",
    "            c_cpu = tf.matmul(a_cpu, b_cpu)\n",
    "            cpu_time = time.time() - start\n",
    "            results['cpu_times'].append(cpu_time)\n",
    "            print(f\"  CPU time: {cpu_time:.4f} seconds\")\n",
    "        \n",
    "        # GPU test\n",
    "        with tf.device('/gpu:0'):\n",
    "            a_gpu = tf.random.normal([size, size])\n",
    "            b_gpu = tf.random.normal([size, size])\n",
    "            \n",
    "            # Warm-up run\n",
    "            c_gpu = tf.matmul(a_gpu, b_gpu)\n",
    "            \n",
    "            # Timed run\n",
    "            start = time.time()\n",
    "            c_gpu = tf.matmul(a_gpu, b_gpu)\n",
    "            gpu_time = time.time() - start\n",
    "            results['gpu_times'].append(gpu_time)\n",
    "            print(f\"  GPU time: {gpu_time:.4f} seconds\")\n",
    "        \n",
    "        # Calculate speedup\n",
    "        speedup = cpu_time / gpu_time if gpu_time > 0 else float('inf')\n",
    "        results['speedups'].append(speedup)\n",
    "        print(f\"  Speedup: {speedup:.2f}x\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(sizes, results['cpu_times'], 'b-o', label='CPU')\n",
    "    plt.plot(sizes, results['gpu_times'], 'r-o', label='GPU')\n",
    "    plt.xlabel('Matrix Size')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('TensorFlow Matrix Multiplication Performance')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(sizes, results['speedups'], 'g-o')\n",
    "    plt.xlabel('Matrix Size')\n",
    "    plt.ylabel('Speedup (CPU time / GPU time)')\n",
    "    plt.title('GPU Speedup Factor')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to run the test\n",
    "# results_tf = test_tensorflow_performance(sizes=[1000, 2000, 4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792960f3",
   "metadata": {},
   "source": [
    "## 4. Memory Usage Optimization Tests\n",
    "\n",
    "Test different memory optimization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e47354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_memory_optimization():\n",
    "    if not TORCH_AVAILABLE or not torch.cuda.is_available():\n",
    "        print(\"PyTorch or CUDA not available. Skipping test.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nTesting GPU memory usage optimization strategies...\")\n",
    "    \n",
    "    # Base memory usage\n",
    "    torch.cuda.empty_cache()\n",
    "    base_memory = torch.cuda.memory_allocated() / 1024 / 1024\n",
    "    print(f\"Base GPU memory usage: {base_memory:.2f} MB\")\n",
    "    \n",
    "    # Test 1: Standard vs Mixed Precision\n",
    "    print(\"\\nTest 1: Standard vs Mixed Precision\")\n",
    "    size = 5000\n",
    "    \n",
    "    # Standard precision (float32)\n",
    "    torch.cuda.empty_cache()\n",
    "    start_memory = torch.cuda.memory_allocated() / 1024 / 1024\n",
    "    start_time = time.time()\n",
    "    \n",
    "    a = torch.rand(size, size, device='cuda')\n",
    "    b = torch.rand(size, size, device='cuda')\n",
    "    c = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    end_memory = torch.cuda.memory_allocated() / 1024 / 1024\n",
    "    \n",
    "    fp32_time = end_time - start_time\n",
    "    fp32_memory = end_memory - start_memory\n",
    "    \n",
    "    print(f\"FP32: Time = {fp32_time:.4f}s, Memory = {fp32_memory:.2f} MB\")\n",
    "    \n",
    "    # Free memory\n",
    "    del a, b, c\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Mixed precision (float16)\n",
    "    torch.cuda.empty_cache()\n",
    "    start_memory = torch.cuda.memory_allocated() / 1024 / 1024\n",
    "    start_time = time.time()\n",
    "    \n",
    "    a = torch.rand(size, size, device='cuda', dtype=torch.float16)\n",
    "    b = torch.rand(size, size, device='cuda', dtype=torch.float16)\n",
    "    c = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    end_memory = torch.cuda.memory_allocated() / 1024 / 1024\n",
    "    \n",
    "    fp16_time = end_time - start_time\n",
    "    fp16_memory = end_memory - start_memory\n",
    "    \n",
    "    print(f\"FP16: Time = {fp16_time:.4f}s, Memory = {fp16_memory:.2f} MB\")\n",
    "    print(f\"Speedup: {fp32_time / fp16_time:.2f}x, Memory saving: {(1 - fp16_memory / fp32_memory) * 100:.1f}%\")\n",
    "    \n",
    "    # Free memory\n",
    "    del a, b, c\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Test 2: Gradient accumulation simulation\n",
    "    print(\"\\nTest 2: Gradient Accumulation Simulation\")\n",
    "    \n",
    "    batch_size = 128\n",
    "    model_size = 1000\n",
    "    accumulation_steps = 4\n",
    "    \n",
    "    # Standard approach (large batch)\n",
    "    torch.cuda.empty_cache()\n",
    "    start_memory = torch.cuda.memory_allocated() / 1024 / 1024\n",
    "    \n",
    "    # Simulate a large batch\n",
    "    big_batch = torch.rand(batch_size * accumulation_steps, model_size, device='cuda')\n",
    "    weights = torch.rand(model_size, model_size, device='cuda')\n",
    "    output = torch.matmul(big_batch, weights)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    end_memory = torch.cuda.memory_allocated() / 1024 / 1024\n",
    "    big_batch_memory = end_memory - start_memory\n",
    "    \n",
    "    print(f\"Large batch approach: Memory = {big_batch_memory:.2f} MB\")\n",
    "    \n",
    "    # Free memory\n",
    "    del big_batch, weights, output, loss\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Gradient accumulation approach\n",
    "    torch.cuda.empty_cache()\n",
    "    start_memory = torch.cuda.memory_allocated() / 1024 / 1024\n",
    "    peak_memory = start_memory\n",
    "    \n",
    "    weights = torch.rand(model_size, model_size, device='cuda')\n",
    "    weights.requires_grad = True\n",
    "    \n",
    "    for i in range(accumulation_steps):\n",
    "        small_batch = torch.rand(batch_size, model_size, device='cuda')\n",
    "        output = torch.matmul(small_batch, weights)\n",
    "        loss = output.sum() / accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        current_memory = torch.cuda.memory_allocated() / 1024 / 1024\n",
    "        peak_memory = max(peak_memory, current_memory)\n",
    "        \n",
    "        # Free intermediate tensors\n",
    "        del small_batch, output, loss\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    accum_memory = peak_memory - start_memory\n",
    "    \n",
    "    print(f\"Gradient accumulation approach: Memory = {accum_memory:.2f} MB\")\n",
    "    print(f\"Memory saving: {(1 - accum_memory / big_batch_memory) * 100:.1f}%\")\n",
    "    \n",
    "    # Free memory\n",
    "    del weights\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Uncomment to run the test\n",
    "# test_memory_optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe10c1",
   "metadata": {},
   "source": [
    "## 5. System-specific Optimization Recommendations\n",
    "\n",
    "Generate optimization recommendations based on the testing results and system configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a11d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_optimization_recommendations():\n",
    "    recommendations = []\n",
    "    \n",
    "    # GPU availability recommendations\n",
    "    if gpu_manager.check_gpu_availability():\n",
    "        gpu_info = gpu_manager.get_gpu_info()\n",
    "        total_gpu_memory = sum(gpu.get('memory_total_mb', 0) for gpu in gpu_info)\n",
    "        \n",
    "        # GPU memory recommendations\n",
    "        if total_gpu_memory < 8000:  # Less than 8GB\n",
    "            recommendations.append(\"Limited GPU memory detected. Consider these optimizations:\")\n",
    "            recommendations.append(\"- Use mixed precision (FP16) training when possible\")\n",
    "            recommendations.append(\"- Implement gradient accumulation for large models\")\n",
    "            recommendations.append(\"- Reduce batch sizes and increase accumulation steps\")\n",
    "        elif total_gpu_memory < 16000:  # Between 8GB and 16GB\n",
    "            recommendations.append(\"Moderate GPU memory available. Consider these optimizations:\")\n",
    "            recommendations.append(\"- Use mixed precision for models larger than 100M parameters\")\n",
    "            recommendations.append(\"- Monitor memory usage during training with the system monitoring utilities\")\n",
    "        else:  # More than 16GB\n",
    "            recommendations.append(\"Large GPU memory available. Optimize for speed:\")\n",
    "            recommendations.append(\"- Increase batch sizes for better throughput\")\n",
    "            recommendations.append(\"- Use model parallelism for very large models\")\n",
    "            \n",
    "        # Multi-GPU recommendations\n",
    "        if len(gpu_info) > 1:\n",
    "            recommendations.append(\"\\nMultiple GPUs detected. Consider these strategies:\")\n",
    "            recommendations.append(\"- Use DataParallel/DistributedDataParallel for training\")\n",
    "            recommendations.append(\"- Allocate different experiments to different GPUs\")\n",
    "            recommendations.append(\"- Use the NVIDIA monitoring tools to track individual GPU usage\")\n",
    "    else:\n",
    "        recommendations.append(\"No GPU detected. For improved performance:\")\n",
    "        recommendations.append(\"- Consider running on a GPU-enabled system for deep learning tasks\")\n",
    "        recommendations.append(\"- Optimize CPU usage by setting proper thread counts\")\n",
    "        recommendations.append(\"- Use smaller models or quantized versions when possible\")\n",
    "    \n",
    "    # Framework-specific recommendations\n",
    "    if 'torch' in sys.modules and torch.cuda.is_available():\n",
    "        recommendations.append(\"\\nPyTorch-specific optimizations:\")\n",
    "        recommendations.append(\"- Use torch.cuda.amp.autocast for automatic mixed precision\")\n",
    "        recommendations.append(\"- Implement torch.cuda.empty_cache() between large operations\")\n",
    "        recommendations.append(\"- Consider torch.backends.cudnn.benchmark=True for repeated operations\")\n",
    "    \n",
    "    if 'tensorflow' in sys.modules and len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "        recommendations.append(\"\\nTensorFlow-specific optimizations:\")\n",
    "        recommendations.append(\"- Use tf.config.experimental.set_memory_growth(gpu, True) to avoid memory spikes\")\n",
    "        recommendations.append(\"- Enable mixed precision with tf.keras.mixed_precision.set_global_policy('mixed_float16')\")\n",
    "        recommendations.append(\"- Use TF datasets with prefetch for better GPU utilization\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Get and display optimization recommendations\n",
    "recommendations = generate_optimization_recommendations()\n",
    "print(\"GPU Optimization Recommendations\\n\" + \"-\"*30)\n",
    "for rec in recommendations:\n",
    "    print(rec)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
